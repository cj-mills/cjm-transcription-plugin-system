{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plugin interface\n",
    "\n",
    "> Abstract base class defining the interface for transcription plugins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp plugin_interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from abc import ABC, abstractmethod\n",
    "import logging\n",
    "from typing import Optional, Dict, Any, Union, List, Tuple, Generator\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "try:\n",
    "    import jsonschema\n",
    "    HAS_JSONSCHEMA = True\n",
    "except ImportError:\n",
    "    HAS_JSONSCHEMA = False\n",
    "from cjm_transcription_plugin_system.core import AudioData, TranscriptionResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PluginInterface(ABC):\n",
    "    \"\"\"Base interface that all transcription plugins must implement.\"\"\"\n",
    "    \n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def name(\n",
    "        self\n",
    "    ) -> str:  # The unique identifier for this plugin\n",
    "        \"\"\"Unique plugin identifier.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def version(\n",
    "        self\n",
    "    ) -> str:  # The semantic version string (e.g., \"1.0.0\")\n",
    "        \"\"\"Plugin version.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def supported_formats(\n",
    "        self\n",
    "    ) -> List[str]:  # List of file extensions this plugin can process\n",
    "        \"\"\"List of supported audio formats (e.g., ['wav', 'mp3']).\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def initialize(\n",
    "        self,\n",
    "        config: Optional[Dict[str, Any]] = None  # Configuration dictionary for plugin-specific settings\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the plugin with configuration.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def execute(\n",
    "        self,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ) -> Any:  # Returns transcription result or plugin-specific output\n",
    "        \"\"\"Transcribe audio to text.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def is_available(\n",
    "        self\n",
    "    ) -> bool:  # True if all required dependencies are available\n",
    "        \"\"\"Check if the plugin's dependencies are available.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_config_schema(\n",
    "        self\n",
    "    ) -> Dict[str, Any]:  # JSON Schema describing configuration options\n",
    "        \"\"\"Return JSON Schema describing the plugin's configuration options.\n",
    "        \n",
    "        The schema should follow JSON Schema Draft 7 specification.\n",
    "        This enables automatic UI generation and validation.\n",
    "        \n",
    "        Example:\n",
    "            {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"model\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"tiny\", \"base\", \"small\"],\n",
    "                        \"default\": \"base\",\n",
    "                        \"description\": \"Model size to use\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"model\"]\n",
    "            }\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_current_config(\n",
    "        self\n",
    "    ) -> Dict[str, Any]:  # Current configuration state\n",
    "        \"\"\"Return the current configuration state.\n",
    "        \n",
    "        This should return the actual configuration being used by the plugin,\n",
    "        which may include defaults not explicitly set by the user.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def validate_config(\n",
    "        self,\n",
    "        config: Dict[str, Any]  # Configuration to validate\n",
    "    ) -> Tuple[bool, Optional[str]]:  # (is_valid, error_message)\n",
    "        \"\"\"Validate a configuration dictionary against the schema.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (is_valid, error_message).\n",
    "            If valid, error_message is None.\n",
    "        \"\"\"\n",
    "        schema = self.get_config_schema()\n",
    "        \n",
    "        # If jsonschema is available, use it for validation\n",
    "        if HAS_JSONSCHEMA:\n",
    "            try:\n",
    "                jsonschema.validate(instance=config, schema=schema)\n",
    "                return True, None\n",
    "            except jsonschema.exceptions.ValidationError as e:\n",
    "                return False, str(e)\n",
    "            except Exception as e:\n",
    "                return False, f\"Validation error: {str(e)}\"\n",
    "        else:\n",
    "            # Basic validation without jsonschema\n",
    "            try:\n",
    "                # Check required fields\n",
    "                required_fields = schema.get(\"required\", [])\n",
    "                for field in required_fields:\n",
    "                    if field not in config:\n",
    "                        return False, f\"Missing required field: {field}\"\n",
    "                \n",
    "                # Check field types if properties are defined\n",
    "                properties = schema.get(\"properties\", {})\n",
    "                for key, value in config.items():\n",
    "                    if key in properties:\n",
    "                        prop_schema = properties[key]\n",
    "                        \n",
    "                        # Check enum values\n",
    "                        if \"enum\" in prop_schema and value not in prop_schema[\"enum\"]:\n",
    "                            return False, f\"Invalid value for {key}: {value}. Must be one of {prop_schema['enum']}\"\n",
    "                        \n",
    "                        # Basic type checking\n",
    "                        expected_type = prop_schema.get(\"type\")\n",
    "                        if expected_type:\n",
    "                            type_map = {\n",
    "                                \"string\": str,\n",
    "                                \"number\": (int, float),\n",
    "                                \"integer\": int,\n",
    "                                \"boolean\": bool,\n",
    "                                \"array\": list,\n",
    "                                \"object\": dict\n",
    "                            }\n",
    "                            expected_python_type = type_map.get(expected_type)\n",
    "                            if expected_python_type and not isinstance(value, expected_python_type):\n",
    "                                return False, f\"Invalid type for {key}: expected {expected_type}, got {type(value).__name__}\"\n",
    "                        \n",
    "                        # Check numeric constraints\n",
    "                        if isinstance(value, (int, float)):\n",
    "                            if \"minimum\" in prop_schema and value < prop_schema[\"minimum\"]:\n",
    "                                return False, f\"Value for {key} is below minimum: {value} < {prop_schema['minimum']}\"\n",
    "                            if \"maximum\" in prop_schema and value > prop_schema[\"maximum\"]:\n",
    "                                return False, f\"Value for {key} is above maximum: {value} > {prop_schema['maximum']}\"\n",
    "                \n",
    "                return True, None\n",
    "            except Exception as e:\n",
    "                return False, f\"Validation error: {str(e)}\"\n",
    "    \n",
    "    def get_config_defaults(\n",
    "        self\n",
    "    ) -> Dict[str, Any]:  # Default values from schema\n",
    "        \"\"\"Extract default values from the configuration schema.\n",
    "        \n",
    "        Returns a dictionary of default values for all properties\n",
    "        that have defaults defined in the schema.\n",
    "        \"\"\"\n",
    "        schema = self.get_config_schema()\n",
    "        defaults = {}\n",
    "        \n",
    "        properties = schema.get(\"properties\", {})\n",
    "        for key, prop_schema in properties.items():\n",
    "            if \"default\" in prop_schema:\n",
    "                defaults[key] = prop_schema[\"default\"]\n",
    "        \n",
    "        return defaults\n",
    "    \n",
    "    def cleanup(\n",
    "        self\n",
    "    ) -> None:\n",
    "        \"\"\"Optional cleanup when plugin is unloaded.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Add streaming support methods to PluginInterface\n",
    "def PluginInterface_supports_streaming(self) -> bool:\n",
    "    \"\"\"Check if this plugin supports streaming transcription.\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if execute_stream is implemented and streaming is supported\n",
    "    \"\"\"\n",
    "    # Default: check if execute_stream is overridden from the base class\n",
    "    return type(self).execute_stream != PluginInterface.execute_stream\n",
    "\n",
    "def PluginInterface_execute_stream(\n",
    "    self,\n",
    "    audio: Union[AudioData, str, Path],  # Audio data or path to audio file\n",
    "    **kwargs  # Additional plugin-specific parameters\n",
    ") -> Generator[str, None, TranscriptionResult]:  # Yields text chunks, returns final result\n",
    "    \"\"\"Stream transcription results chunk by chunk.\n",
    "    \n",
    "    Default implementation falls back to execute() without streaming.\n",
    "    Plugins can override this to provide real streaming capabilities.\n",
    "    \n",
    "    Args:\n",
    "        audio: Audio data or path to audio file\n",
    "        **kwargs: Additional plugin-specific parameters\n",
    "        \n",
    "    Yields:\n",
    "        str: Partial transcription text chunks as they become available\n",
    "        \n",
    "    Returns:\n",
    "        TranscriptionResult: Final complete transcription with metadata\n",
    "        \n",
    "    Example:\n",
    "        >>> # Stream transcription chunks in real-time\n",
    "        >>> for chunk in plugin.execute_stream(audio_file):\n",
    "        ...     print(chunk, end=\"\", flush=True)\n",
    "        >>> \n",
    "        >>> # Or collect all chunks and get final result\n",
    "        >>> generator = plugin.execute_stream(audio_file)\n",
    "        >>> chunks = []\n",
    "        >>> for chunk in generator:\n",
    "        ...     chunks.append(chunk)\n",
    "        >>> result = generator.value  # Final TranscriptionResult\n",
    "    \"\"\"\n",
    "    # Default implementation: execute normally and yield complete result at once\n",
    "    result = self.execute(audio, **kwargs)\n",
    "    \n",
    "    # If result is a TranscriptionResult, yield its text\n",
    "    if hasattr(result, 'text'):\n",
    "        yield result.text\n",
    "        return result\n",
    "    else:\n",
    "        # For plugins that don't return TranscriptionResult\n",
    "        yield str(result)\n",
    "        return TranscriptionResult(text=str(result))\n",
    "\n",
    "# Add the methods to the PluginInterface class\n",
    "PluginInterface.supports_streaming = PluginInterface_supports_streaming\n",
    "PluginInterface.execute_stream = PluginInterface_execute_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class PluginMeta:\n",
    "    \"\"\"Metadata about a plugin.\"\"\"\n",
    "    name: str  # The plugin's unique identifier\n",
    "    version: str  # The plugin's version string\n",
    "    description: str = \"\"  # A brief description of the plugin's functionality\n",
    "    author: str = \"\"  # The plugin author's name or organization\n",
    "    package_name: str = \"\"  # The Python package name containing the plugin\n",
    "    instance: Optional[PluginInterface] = None  # The plugin instance\n",
    "    enabled: bool = True  # Whether the plugin is enabled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing PluginMeta dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PluginMeta instance:\n",
      "PluginMeta(name='test_plugin', version='1.0.0', description='A test plugin', author='Test Author', package_name='', instance=None, enabled=True)\n",
      "\n",
      "Name: test_plugin\n",
      "Version: 1.0.0\n",
      "Enabled: True\n",
      "Instance: None\n",
      "\n",
      "Minimal PluginMeta: PluginMeta(name='minimal', version='0.1.0', description='', author='', package_name='', instance=None, enabled=True)\n",
      "Equality test: True\n"
     ]
    }
   ],
   "source": [
    "# Test PluginMeta dataclass\n",
    "meta = PluginMeta(\n",
    "    name=\"test_plugin\",\n",
    "    version=\"1.0.0\",\n",
    "    description=\"A test plugin\",\n",
    "    author=\"Test Author\"\n",
    ")\n",
    "\n",
    "print(\"PluginMeta instance:\")\n",
    "print(meta)\n",
    "print(f\"\\nName: {meta.name}\")\n",
    "print(f\"Version: {meta.version}\")\n",
    "print(f\"Enabled: {meta.enabled}\")\n",
    "print(f\"Instance: {meta.instance}\")\n",
    "\n",
    "# Test with minimal arguments\n",
    "minimal_meta = PluginMeta(name=\"minimal\", version=\"0.1.0\")\n",
    "print(f\"\\nMinimal PluginMeta: {minimal_meta}\")\n",
    "\n",
    "# Test equality\n",
    "meta_copy = PluginMeta(name=\"minimal\", version=\"0.1.0\")\n",
    "print(f\"Equality test: {minimal_meta == meta_copy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExamplePlugin(PluginInterface):\n",
    "    \"\"\"An example plugin implementation with configuration schema.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(f\"{__name__}.{type(self).__name__}\")\n",
    "        self.config = {}\n",
    "        self.model = None\n",
    "    \n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return \"example_plugin\"\n",
    "    \n",
    "    @property\n",
    "    def version(self) -> str:\n",
    "        return \"1.0.0\"\n",
    "\n",
    "    @property\n",
    "    def supported_formats(self) -> List[str]:\n",
    "        return [\"wav\", \"mp3\", \"flac\"]\n",
    "    \n",
    "    def get_config_schema(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return the configuration schema for this plugin.\"\"\"\n",
    "        return {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"model\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"tiny\", \"base\", \"small\", \"medium\", \"large\"],\n",
    "                    \"default\": \"base\",\n",
    "                    \"description\": \"Model size to use for transcription\"\n",
    "                },\n",
    "                \"language\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"default\": \"auto\",\n",
    "                    \"description\": \"Language code (e.g., 'en', 'es') or 'auto' for detection\"\n",
    "                },\n",
    "                \"batch_size\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"minimum\": 1,\n",
    "                    \"maximum\": 32,\n",
    "                    \"default\": 8,\n",
    "                    \"description\": \"Batch size for processing\"\n",
    "                },\n",
    "                \"enable_vad\": {\n",
    "                    \"type\": \"boolean\",\n",
    "                    \"default\": True,\n",
    "                    \"description\": \"Enable voice activity detection\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"model\"]\n",
    "        }\n",
    "    \n",
    "    def get_current_config(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return the current configuration.\"\"\"\n",
    "        # Merge defaults with actual config\n",
    "        defaults = self.get_config_defaults()\n",
    "        return {**defaults, **self.config}\n",
    "    \n",
    "    def initialize(self, config: Optional[Dict[str, Any]] = None) -> None:\n",
    "        \"\"\"Initialize the plugin.\"\"\"\n",
    "        if config:\n",
    "            is_valid, error = self.validate_config(config)\n",
    "            if not is_valid:\n",
    "                raise ValueError(f\"Invalid configuration: {error}\")\n",
    "        \n",
    "        # Merge provided config with defaults\n",
    "        defaults = self.get_config_defaults()\n",
    "        self.config = {**defaults, **(config or {})}\n",
    "        \n",
    "        self.logger.info(f\"Initializing {self.name} with config: {self.config}\")\n",
    "        \n",
    "        # Simulate loading a model based on config\n",
    "        model_name = self.config.get(\"model\", \"base\")\n",
    "        self.model = f\"MockModel-{model_name}\"\n",
    "    \n",
    "    def execute(self, *args, **kwargs) -> Any:\n",
    "        \"\"\"Execute the plugin's functionality.\"\"\"\n",
    "        self.logger.info(f\"Example plugin executed with model: {self.model}\")\n",
    "        self.logger.info(f\"Config: {self.config}\")\n",
    "        return f\"Transcription using {self.model}\"\n",
    "\n",
    "    def is_available(self) -> bool:\n",
    "        \"\"\"Check availability.\"\"\"\n",
    "        return True\n",
    "    \n",
    "    def cleanup(self) -> None:\n",
    "        \"\"\"Clean up resources.\"\"\"\n",
    "        self.logger.info(f\"Cleaning up {self.name}\")\n",
    "        self.model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__.ExamplePlugin:Initializing example_plugin with config: {'model': 'base', 'language': 'auto', 'batch_size': 8, 'enable_vad': True}\n",
      "INFO:__main__.ExamplePlugin:Example plugin executed with model: MockModel-base\n",
      "INFO:__main__.ExamplePlugin:Config: {'model': 'base', 'language': 'auto', 'batch_size': 8, 'enable_vad': True}\n",
      "INFO:__main__.ExamplePlugin:Cleaning up example_plugin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription using MockModel-base\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "example_plugin = ExamplePlugin()\n",
    "example_plugin.initialize()\n",
    "transcription_result = example_plugin.execute(\"test_audio.mp3\")\n",
    "example_plugin.cleanup()\n",
    "print(transcription_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__.ExamplePlugin:Initializing example_plugin with config: {'model': 'small', 'language': 'en', 'batch_size': 8, 'enable_vad': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration Schema:\n",
      "{\n",
      "  \"type\": \"object\",\n",
      "  \"properties\": {\n",
      "    \"model\": {\n",
      "      \"type\": \"string\",\n",
      "      \"enum\": [\n",
      "        \"tiny\",\n",
      "        \"base\",\n",
      "        \"small\",\n",
      "        \"medium\",\n",
      "        \"large\"\n",
      "      ],\n",
      "      \"default\": \"base\",\n",
      "      \"description\": \"Model size to use for transcription\"\n",
      "    },\n",
      "    \"language\": {\n",
      "      \"type\": \"string\",\n",
      "      \"default\": \"auto\",\n",
      "      \"description\": \"Language code (e.g., 'en', 'es') or 'auto' for detection\"\n",
      "    },\n",
      "    \"batch_size\": {\n",
      "      \"type\": \"integer\",\n",
      "      \"minimum\": 1,\n",
      "      \"maximum\": 32,\n",
      "      \"default\": 8,\n",
      "      \"description\": \"Batch size for processing\"\n",
      "    },\n",
      "    \"enable_vad\": {\n",
      "      \"type\": \"boolean\",\n",
      "      \"default\": true,\n",
      "      \"description\": \"Enable voice activity detection\"\n",
      "    }\n",
      "  },\n",
      "  \"required\": [\n",
      "    \"model\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "==================================================\n",
      "\n",
      "Default Configuration:\n",
      "{\n",
      "  \"model\": \"base\",\n",
      "  \"language\": \"auto\",\n",
      "  \"batch_size\": 8,\n",
      "  \"enable_vad\": true\n",
      "}\n",
      "\n",
      "==================================================\n",
      "\n",
      "Current Configuration after initialization:\n",
      "{\n",
      "  \"model\": \"small\",\n",
      "  \"language\": \"en\",\n",
      "  \"batch_size\": 8,\n",
      "  \"enable_vad\": true\n",
      "}\n",
      "\n",
      "==================================================\n",
      "\n",
      "Valid config with minimal settings:\n",
      "  Config: {'model': 'tiny'}\n",
      "  Valid: True\n",
      "\n",
      "Invalid model name:\n",
      "  Config: {'model': 'invalid_model'}\n",
      "  Valid: False\n",
      "  Error: 'invalid_model' is not one of ['tiny', 'base', 'small', 'medium', 'large']\n",
      "\n",
      "Failed validating 'enum' in schema['properties']['model']:\n",
      "    {'type': 'string',\n",
      "     'enum': ['tiny', 'base', 'small', 'medium', 'large'],\n",
      "     'default': 'base',\n",
      "     'description': 'Model size to use for transcription'}\n",
      "\n",
      "On instance['model']:\n",
      "    'invalid_model'\n",
      "\n",
      "Missing required 'model' field:\n",
      "  Config: {'batch_size': 100}\n",
      "  Valid: False\n",
      "  Error: 'model' is a required property\n",
      "\n",
      "Failed validating 'required' in schema:\n",
      "    {'type': 'object',\n",
      "     'properties': {'model': {'type': 'string',\n",
      "                              'enum': ['tiny',\n",
      "                                       'base',\n",
      "                                       'small',\n",
      "                                       'medium',\n",
      "                                       'large'],\n",
      "                              'default': 'base',\n",
      "                              'description': 'Model size to use for '\n",
      "                                             'transcription'},\n",
      "                    'language': {'type': 'string',\n",
      "                                 'default': 'auto',\n",
      "                                 'description': 'Language code (e.g., '\n",
      "                                                \"'en', 'es') or 'auto' for \"\n",
      "                                                'detection'},\n",
      "                    'batch_size': {'type': 'integer',\n",
      "                                   'minimum': 1,\n",
      "                                   'maximum': 32,\n",
      "                                   'default': 8,\n",
      "                                   'description': 'Batch size for '\n",
      "                                                  'processing'},\n",
      "                    'enable_vad': {'type': 'boolean',\n",
      "                                   'default': True,\n",
      "                                   'description': 'Enable voice activity '\n",
      "                                                  'detection'}},\n",
      "     'required': ['model']}\n",
      "\n",
      "On instance:\n",
      "    {'batch_size': 100}\n",
      "\n",
      "Batch size exceeds maximum:\n",
      "  Config: {'model': 'base', 'batch_size': 100}\n",
      "  Valid: False\n",
      "  Error: 100 is greater than the maximum of 32\n",
      "\n",
      "Failed validating 'maximum' in schema['properties']['batch_size']:\n",
      "    {'type': 'integer',\n",
      "     'minimum': 1,\n",
      "     'maximum': 32,\n",
      "     'default': 8,\n",
      "     'description': 'Batch size for processing'}\n",
      "\n",
      "On instance['batch_size']:\n",
      "    100\n",
      "\n",
      "Invalid type for batch_size:\n",
      "  Config: {'model': 'base', 'batch_size': 'not_a_number'}\n",
      "  Valid: False\n",
      "  Error: 'not_a_number' is not of type 'integer'\n",
      "\n",
      "Failed validating 'type' in schema['properties']['batch_size']:\n",
      "    {'type': 'integer',\n",
      "     'minimum': 1,\n",
      "     'maximum': 32,\n",
      "     'default': 8,\n",
      "     'description': 'Batch size for processing'}\n",
      "\n",
      "On instance['batch_size']:\n",
      "    'not_a_number'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the configuration schema functionality\n",
    "plugin = ExamplePlugin()\n",
    "\n",
    "# Get the configuration schema\n",
    "schema = plugin.get_config_schema()\n",
    "print(\"Configuration Schema:\")\n",
    "print(json.dumps(schema, indent=2))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Get default configuration\n",
    "defaults = plugin.get_config_defaults()\n",
    "print(\"Default Configuration:\")\n",
    "print(json.dumps(defaults, indent=2))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Initialize with partial config (using defaults for missing values)\n",
    "plugin.initialize({\"model\": \"small\", \"language\": \"en\"})\n",
    "print(\"Current Configuration after initialization:\")\n",
    "print(json.dumps(plugin.get_current_config(), indent=2))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Test configuration validation\n",
    "test_configs = [\n",
    "    ({\"model\": \"tiny\"}, \"Valid config with minimal settings\"),\n",
    "    ({\"model\": \"invalid_model\"}, \"Invalid model name\"),\n",
    "    ({\"batch_size\": 100}, \"Missing required 'model' field\"),\n",
    "    ({\"model\": \"base\", \"batch_size\": 100}, \"Batch size exceeds maximum\"),\n",
    "    ({\"model\": \"base\", \"batch_size\": \"not_a_number\"}, \"Invalid type for batch_size\"),\n",
    "]\n",
    "\n",
    "for config, description in test_configs:\n",
    "    is_valid, error = plugin.validate_config(config)\n",
    "    print(f\"{description}:\")\n",
    "    print(f\"  Config: {config}\")\n",
    "    print(f\"  Valid: {is_valid}\")\n",
    "    if error:\n",
    "        print(f\"  Error: {error}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Configuration Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Whisper Plugin Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhisperPlugin(PluginInterface):\n",
    "    \"\"\"Example Whisper transcription plugin with comprehensive configuration.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(f\"{__name__}.{type(self).__name__}\")\n",
    "        self.config = {}\n",
    "        self.model = None\n",
    "        self.processor = None\n",
    "    \n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return \"whisper\"\n",
    "    \n",
    "    @property\n",
    "    def version(self) -> str:\n",
    "        return \"1.0.0\"\n",
    "    \n",
    "    @property\n",
    "    def supported_formats(self) -> List[str]:\n",
    "        return [\"wav\", \"mp3\", \"flac\", \"m4a\", \"ogg\", \"webm\"]\n",
    "    \n",
    "    def get_config_schema(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return comprehensive Whisper configuration schema.\"\"\"\n",
    "        return {\n",
    "            \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
    "            \"type\": \"object\",\n",
    "            \"title\": \"Whisper Configuration\",\n",
    "            \"properties\": {\n",
    "                \"model\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"tiny\", \"tiny.en\", \"base\", \"base.en\", \"small\", \"small.en\", \n",
    "                            \"medium\", \"medium.en\", \"large\", \"large-v1\", \"large-v2\", \"large-v3\"],\n",
    "                    \"default\": \"base\",\n",
    "                    \"description\": \"Whisper model size. Larger models are more accurate but slower.\"\n",
    "                },\n",
    "                \"device\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"cpu\", \"cuda\", \"mps\", \"auto\"],\n",
    "                    \"default\": \"auto\",\n",
    "                    \"description\": \"Computation device for inference\"\n",
    "                },\n",
    "                \"compute_type\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"default\", \"float16\", \"float32\", \"int8\", \"int8_float16\"],\n",
    "                    \"default\": \"default\",\n",
    "                    \"description\": \"Model precision/quantization\"\n",
    "                },\n",
    "                \"language\": {\n",
    "                    \"type\": [\"string\", \"null\"],\n",
    "                    \"default\": None,\n",
    "                    \"description\": \"Language code (e.g., 'en', 'es', 'fr') or null for auto-detection\",\n",
    "                    \"examples\": [\"en\", \"es\", \"fr\", \"de\", \"ja\", \"zh\", None]\n",
    "                },\n",
    "                \"task\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"transcribe\", \"translate\"],\n",
    "                    \"default\": \"transcribe\",\n",
    "                    \"description\": \"Task to perform (transcribe keeps original language, translate converts to English)\"\n",
    "                },\n",
    "                \"temperature\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"minimum\": 0.0,\n",
    "                    \"maximum\": 1.0,\n",
    "                    \"default\": 0.0,\n",
    "                    \"description\": \"Sampling temperature. 0 for deterministic, higher values for more variation\"\n",
    "                },\n",
    "                \"beam_size\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"minimum\": 1,\n",
    "                    \"maximum\": 10,\n",
    "                    \"default\": 5,\n",
    "                    \"description\": \"Beam search width. Higher values may improve accuracy but are slower\"\n",
    "                },\n",
    "                \"best_of\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"minimum\": 1,\n",
    "                    \"maximum\": 10,\n",
    "                    \"default\": 5,\n",
    "                    \"description\": \"Number of candidates when sampling with non-zero temperature\"\n",
    "                },\n",
    "                \"patience\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"minimum\": 0.0,\n",
    "                    \"maximum\": 2.0,\n",
    "                    \"default\": 1.0,\n",
    "                    \"description\": \"Beam search patience factor\"\n",
    "                },\n",
    "                \"length_penalty\": {\n",
    "                    \"type\": [\"number\", \"null\"],\n",
    "                    \"default\": None,\n",
    "                    \"description\": \"Exponential length penalty during beam search\"\n",
    "                },\n",
    "                \"suppress_tokens\": {\n",
    "                    \"type\": [\"array\", \"string\"],\n",
    "                    \"items\": {\"type\": \"integer\"},\n",
    "                    \"default\": \"-1\",\n",
    "                    \"description\": \"Token IDs to suppress. '-1' for default suppression, empty array for none\"\n",
    "                },\n",
    "                \"initial_prompt\": {\n",
    "                    \"type\": [\"string\", \"null\"],\n",
    "                    \"default\": None,\n",
    "                    \"description\": \"Optional text to provide as prompt for first window\"\n",
    "                },\n",
    "                \"condition_on_previous_text\": {\n",
    "                    \"type\": \"boolean\",\n",
    "                    \"default\": True,\n",
    "                    \"description\": \"Use previous output as prompt for next window\"\n",
    "                },\n",
    "                \"no_speech_threshold\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"minimum\": 0.0,\n",
    "                    \"maximum\": 1.0,\n",
    "                    \"default\": 0.6,\n",
    "                    \"description\": \"Threshold for detecting silence\"\n",
    "                },\n",
    "                \"compression_ratio_threshold\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"minimum\": 1.0,\n",
    "                    \"maximum\": 10.0,\n",
    "                    \"default\": 2.4,\n",
    "                    \"description\": \"Threshold for detecting repetition\"\n",
    "                },\n",
    "                \"logprob_threshold\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"default\": -1.0,\n",
    "                    \"description\": \"Average log probability threshold\"\n",
    "                },\n",
    "                \"word_timestamps\": {\n",
    "                    \"type\": \"boolean\",\n",
    "                    \"default\": False,\n",
    "                    \"description\": \"Extract word-level timestamps\"\n",
    "                },\n",
    "                \"prepend_punctuations\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"default\": \"\\\"'“¿([{-\",\n",
    "                    \"description\": \"Punctuations to merge with next word\"\n",
    "                },\n",
    "                \"append_punctuations\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"default\": \"\\\"'.。,，!！?？:：”)]}、\",\n",
    "                    \"description\": \"Punctuations to merge with previous word\"\n",
    "                },\n",
    "                \"vad_filter\": {\n",
    "                    \"type\": \"boolean\",\n",
    "                    \"default\": False,\n",
    "                    \"description\": \"Enable voice activity detection filter\"\n",
    "                },\n",
    "                \"vad_parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"threshold\": {\n",
    "                            \"type\": \"number\",\n",
    "                            \"minimum\": 0.0,\n",
    "                            \"maximum\": 1.0,\n",
    "                            \"default\": 0.5\n",
    "                        },\n",
    "                        \"min_speech_duration_ms\": {\n",
    "                            \"type\": \"integer\",\n",
    "                            \"minimum\": 0,\n",
    "                            \"default\": 250\n",
    "                        },\n",
    "                        \"max_speech_duration_s\": {\n",
    "                            \"type\": \"number\",\n",
    "                            \"minimum\": 0,\n",
    "                            \"default\": 3600\n",
    "                        }\n",
    "                    },\n",
    "                    \"default\": {}\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"model\"],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    \n",
    "    def get_current_config(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return current configuration with all defaults applied.\"\"\"\n",
    "        defaults = self.get_config_defaults()\n",
    "        current = {**defaults, **self.config}\n",
    "        \n",
    "        # Handle nested vad_parameters\n",
    "        if \"vad_parameters\" in current and isinstance(current[\"vad_parameters\"], dict):\n",
    "            vad_defaults = {\n",
    "                \"threshold\": 0.5,\n",
    "                \"min_speech_duration_ms\": 250,\n",
    "                \"max_speech_duration_s\": 3600\n",
    "            }\n",
    "            current[\"vad_parameters\"] = {**vad_defaults, **current[\"vad_parameters\"]}\n",
    "        \n",
    "        return current\n",
    "    \n",
    "    def initialize(self, config: Optional[Dict[str, Any]] = None) -> None:\n",
    "        \"\"\"Initialize the Whisper model with configuration.\"\"\"\n",
    "        if config:\n",
    "            is_valid, error = self.validate_config(config)\n",
    "            if not is_valid:\n",
    "                raise ValueError(f\"Invalid configuration: {error}\")\n",
    "        \n",
    "        # Merge with defaults\n",
    "        defaults = self.get_config_defaults()\n",
    "        self.config = {**defaults, **(config or {})}\n",
    "        \n",
    "        self.logger.info(f\"Initializing Whisper with config: {self.config}\")\n",
    "        \n",
    "        # In a real implementation, this would load the actual Whisper model\n",
    "        # For example:\n",
    "        # import whisper\n",
    "        # self.model = whisper.load_model(self.config[\"model\"], device=self.config[\"device\"])\n",
    "        \n",
    "        # Mock implementation\n",
    "        self.model = f\"WhisperModel-{self.config['model']}\"\n",
    "        self.processor = f\"WhisperProcessor-{self.config['device']}\"\n",
    "    \n",
    "    def execute(self, audio_data: Union[AudioData, str, Path], **kwargs) -> TranscriptionResult:\n",
    "        \"\"\"Transcribe audio using Whisper.\"\"\"\n",
    "        if not self.model:\n",
    "            raise RuntimeError(\"Plugin not initialized. Call initialize() first.\")\n",
    "        \n",
    "        # Override config with any provided kwargs\n",
    "        exec_config = {**self.config, **kwargs}\n",
    "        \n",
    "        self.logger.info(f\"Transcribing with Whisper model: {self.model}\")\n",
    "        self.logger.info(f\"Execution config: {exec_config}\")\n",
    "        \n",
    "        # In a real implementation, this would:\n",
    "        # 1. Load/preprocess audio\n",
    "        # 2. Run Whisper inference\n",
    "        # 3. Post-process results\n",
    "        \n",
    "        # Mock transcription result\n",
    "        return TranscriptionResult(\n",
    "            text=f\"Mock transcription using {exec_config['model']} model\",\n",
    "            confidence=0.95,\n",
    "            segments=[\n",
    "                {\n",
    "                    \"start\": 0.0,\n",
    "                    \"end\": 2.5,\n",
    "                    \"text\": \"Mock transcription\",\n",
    "                    \"confidence\": 0.96\n",
    "                },\n",
    "                {\n",
    "                    \"start\": 2.5,\n",
    "                    \"end\": 5.0,\n",
    "                    \"text\": f\"using {exec_config['model']} model\",\n",
    "                    \"confidence\": 0.94\n",
    "                }\n",
    "            ],\n",
    "            metadata={\n",
    "                \"model\": exec_config[\"model\"],\n",
    "                \"language\": exec_config.get(\"language\", \"auto-detected\"),\n",
    "                \"device\": exec_config[\"device\"],\n",
    "                \"task\": exec_config[\"task\"]\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    def is_available(self) -> bool:\n",
    "        \"\"\"Check if Whisper dependencies are available.\"\"\"\n",
    "        try:\n",
    "            # In real implementation, check for whisper package\n",
    "            # import whisper\n",
    "            # return True\n",
    "            return True  # Mock always available\n",
    "        except ImportError:\n",
    "            return False\n",
    "    \n",
    "    def cleanup(self) -> None:\n",
    "        \"\"\"Clean up model from memory.\"\"\"\n",
    "        self.logger.info(\"Cleaning up Whisper model\")\n",
    "        self.model = None\n",
    "        self.processor = None\n",
    "        # In real implementation: del self.model, torch.cuda.empty_cache(), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__.WhisperPlugin:Initializing Whisper with config: {'model': 'tiny', 'device': 'auto', 'compute_type': 'default', 'language': None, 'task': 'transcribe', 'temperature': 0.0, 'beam_size': 5, 'best_of': 5, 'patience': 1.0, 'length_penalty': None, 'suppress_tokens': '-1', 'initial_prompt': None, 'condition_on_previous_text': True, 'no_speech_threshold': 0.6, 'compression_ratio_threshold': 2.4, 'logprob_threshold': -1.0, 'word_timestamps': False, 'prepend_punctuations': '\"\\'“¿([{-', 'append_punctuations': '\"\\'.。,，!！?？:：”)]}、', 'vad_filter': False, 'vad_parameters': {}}\n",
      "INFO:__main__.WhisperPlugin:Transcribing with Whisper model: WhisperModel-tiny\n",
      "INFO:__main__.WhisperPlugin:Execution config: {'model': 'tiny', 'device': 'auto', 'compute_type': 'default', 'language': None, 'task': 'transcribe', 'temperature': 0.0, 'beam_size': 5, 'best_of': 5, 'patience': 1.0, 'length_penalty': None, 'suppress_tokens': '-1', 'initial_prompt': None, 'condition_on_previous_text': True, 'no_speech_threshold': 0.6, 'compression_ratio_threshold': 2.4, 'logprob_threshold': -1.0, 'word_timestamps': False, 'prepend_punctuations': '\"\\'“¿([{-', 'append_punctuations': '\"\\'.。,，!！?？:：”)]}、', 'vad_filter': False, 'vad_parameters': {}}\n",
      "INFO:__main__.WhisperPlugin:Initializing Whisper with config: {'model': 'large-v3', 'device': 'cuda', 'compute_type': 'default', 'language': 'en', 'task': 'transcribe', 'temperature': 0.0, 'beam_size': 5, 'best_of': 5, 'patience': 1.0, 'length_penalty': None, 'suppress_tokens': '-1', 'initial_prompt': None, 'condition_on_previous_text': True, 'no_speech_threshold': 0.6, 'compression_ratio_threshold': 2.4, 'logprob_threshold': -1.0, 'word_timestamps': False, 'prepend_punctuations': '\"\\'“¿([{-', 'append_punctuations': '\"\\'.。,，!！?？:：”)]}、', 'vad_filter': False, 'vad_parameters': {}}\n",
      "INFO:__main__.WhisperPlugin:Transcribing with Whisper model: WhisperModel-large-v3\n",
      "INFO:__main__.WhisperPlugin:Execution config: {'model': 'large-v3', 'device': 'cuda', 'compute_type': 'default', 'language': 'en', 'task': 'transcribe', 'temperature': 0.0, 'beam_size': 5, 'best_of': 5, 'patience': 1.0, 'length_penalty': None, 'suppress_tokens': '-1', 'initial_prompt': None, 'condition_on_previous_text': True, 'no_speech_threshold': 0.6, 'compression_ratio_threshold': 2.4, 'logprob_threshold': -1.0, 'word_timestamps': False, 'prepend_punctuations': '\"\\'“¿([{-', 'append_punctuations': '\"\\'.。,，!！?？:：”)]}、', 'vad_filter': False, 'vad_parameters': {}}\n",
      "INFO:__main__.WhisperPlugin:Initializing Whisper with config: {'model': 'base', 'device': 'auto', 'compute_type': 'default', 'language': None, 'task': 'transcribe', 'temperature': 0.2, 'beam_size': 3, 'best_of': 5, 'patience': 1.0, 'length_penalty': None, 'suppress_tokens': '-1', 'initial_prompt': None, 'condition_on_previous_text': True, 'no_speech_threshold': 0.6, 'compression_ratio_threshold': 2.4, 'logprob_threshold': -1.0, 'word_timestamps': True, 'prepend_punctuations': '\"\\'“¿([{-', 'append_punctuations': '\"\\'.。,，!！?？:：”)]}、', 'vad_filter': False, 'vad_parameters': {}}\n",
      "INFO:__main__.WhisperPlugin:Transcribing with Whisper model: WhisperModel-base\n",
      "INFO:__main__.WhisperPlugin:Execution config: {'model': 'base', 'device': 'auto', 'compute_type': 'default', 'language': None, 'task': 'transcribe', 'temperature': 0.2, 'beam_size': 3, 'best_of': 5, 'patience': 1.0, 'length_penalty': None, 'suppress_tokens': '-1', 'initial_prompt': None, 'condition_on_previous_text': True, 'no_speech_threshold': 0.6, 'compression_ratio_threshold': 2.4, 'logprob_threshold': -1.0, 'word_timestamps': True, 'prepend_punctuations': '\"\\'“¿([{-', 'append_punctuations': '\"\\'.。,，!！?？:：”)]}、', 'vad_filter': False, 'vad_parameters': {}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whisper Configuration Schema (subset):\n",
      "Available models: ['tiny', 'tiny.en', 'base', 'base.en', 'small', 'small.en', 'medium', 'medium.en', 'large', 'large-v1', 'large-v2', 'large-v3']\n",
      "Available devices: ['cpu', 'cuda', 'mps', 'auto']\n",
      "\n",
      "Required fields: ['model']\n",
      "\n",
      "==================================================\n",
      "\n",
      "Initializing with config: {'model': 'tiny'}\n",
      "Current model: tiny\n",
      "Current device: auto\n",
      "Word timestamps: False\n",
      "Result: Mock transcription using tiny model\n",
      "------------------------------\n",
      "Initializing with config: {'model': 'large-v3', 'device': 'cuda', 'language': 'en'}\n",
      "Current model: large-v3\n",
      "Current device: cuda\n",
      "Word timestamps: False\n",
      "Result: Mock transcription using large-v3 model\n",
      "------------------------------\n",
      "Initializing with config: {'model': 'base', 'temperature': 0.2, 'beam_size': 3, 'word_timestamps': True}\n",
      "Current model: base\n",
      "Current device: auto\n",
      "Word timestamps: True\n",
      "Result: Mock transcription using base model\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test the Whisper plugin\n",
    "whisper_plugin = WhisperPlugin()\n",
    "\n",
    "# Get a subset of the schema for display (it's quite large)\n",
    "schema = whisper_plugin.get_config_schema()\n",
    "print(\"Whisper Configuration Schema (subset):\")\n",
    "print(\"Available models:\", schema[\"properties\"][\"model\"][\"enum\"])\n",
    "print(\"Available devices:\", schema[\"properties\"][\"device\"][\"enum\"])\n",
    "print(\"\\nRequired fields:\", schema.get(\"required\", []))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Test initialization with different configurations\n",
    "configs_to_test = [\n",
    "    {\"model\": \"tiny\"},\n",
    "    {\"model\": \"large-v3\", \"device\": \"cuda\", \"language\": \"en\"},\n",
    "    {\"model\": \"base\", \"temperature\": 0.2, \"beam_size\": 3, \"word_timestamps\": True}\n",
    "]\n",
    "\n",
    "for config in configs_to_test:\n",
    "    print(f\"Initializing with config: {config}\")\n",
    "    whisper_plugin.initialize(config)\n",
    "    current = whisper_plugin.get_current_config()\n",
    "    print(f\"Current model: {current['model']}\")\n",
    "    print(f\"Current device: {current['device']}\")\n",
    "    print(f\"Word timestamps: {current['word_timestamps']}\")\n",
    "    \n",
    "    # Execute transcription\n",
    "    result = whisper_plugin.execute(\"dummy_audio.wav\")\n",
    "    print(f\"Result: {result.text}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
